{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XLZdEbAy2jfr"
   },
   "source": [
    "<h1><b>Markov Decision Processes</h1></b>\n",
    "<p align=\"justify\">Στη συγκεκριμένη άσκηση θα μελετήσετε τους αλγορίθμους <i>Policy Iteration</i> και <i>Value Iteration</i>, καθώς και θα εξοικειωθείτε με βασικές έννοιες των <i>Markov Decision Processes</i>. Οι αλγόριθμοι <i>Policy Iteration</i> και <i>Value Iteration</i> είναι από τους βασικούς αλγορίθμους δυναμικού προγραμματισμού που χρησιμοποιούνται για την επίλυση της εξίσωσης <i>Bellman</i> σε <i>Markov Decision Processes</i>.</p> \n",
    "<p align=\"justify\">Το πρόβλημα που θα μελετήσετε είναι αυτό της παγωμένης λίμνης (Frozen Lake) με μέγεθος πλέγματος 8 x 8.</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6VsUV229__zO"
   },
   "source": [
    "<h2><b>Εξοικείωση με τη βιβλιοθήκη <i>Gym</i></b></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OM8ivgOJAg_H"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from gym import wrappers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_puV3ugeAnbU"
   },
   "source": [
    "Με την παρακάτω εντολή, ορίζετε το πρόβλημα που θα μελετηθεί:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ep-MvIUCAxT8"
   },
   "outputs": [],
   "source": [
    "env_name = 'FrozenLake8x8-v0'\n",
    "env = gym.make(env_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uBKBXJDUBRUh"
   },
   "source": [
    "Με τις παρακάτω εντολές, θα επαναφέρετε τον Agent στην αρχική του θέση και θα οπτικοποιήσετε το πλέγμα και τη θέση του Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p6lqbG9zBgdi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FX2res4JBlYb"
   },
   "source": [
    "Με τις παρακάτω εντολές, ορίζετε την επόμενη ενέργεια με τυχαίο τρόπο και ο Agent κάνει ένα βήμα."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gq7q944YBx0Q"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Left)\n",
      "SFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n"
     ]
    }
   ],
   "source": [
    "next_action = env.action_space.sample()\n",
    "#print(next_action) # 0 left , 1 down , 2 right, 3 up      \n",
    "env.step(next_action)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 0.0, False, {'prob': 0.3333333333333333})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_action = env.action_space.sample()\n",
    "env.step(next_action ) # probability 0.33 , # False is if move was done or not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mV4A7lsLB54y"
   },
   "source": [
    "__Ερώτηση__\n",
    "\n",
    "Να εκτελέσετε αρκετές φορές τις τελευταίες εντολές και να παρατηρήσετε κάθε φορά την ενέργεια που ζητείται από τον agent να εκτελέσει και την ενέργεια που αυτός πραγματοποιεί. Πραγματοποιεί πάντοτε ο agent την κίνηση που του ζητείται; Είναι ντετερμινιστικές ή στοχαστικές οι κινήσεις του agent;\n",
    "\n",
    "\n",
    "__Απάντηση__\n",
    "\n",
    "Παρατηρούμε πως ο Agent δεν πάει σε όποια κατεύθυνση του δηλώσουμε. Επομένως οι κινήσεις του δεν είναι ντετερμινιστικές αλλά στοχαστικές. Συγκεκριμένα η εντολή env.step κάνει το βήμα με πιθανότητα 1/3.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PAL4we3gDV_J"
   },
   "source": [
    "<h2><b>Ερωτήσεις</b></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zQKm4VAUChi1"
   },
   "source": [
    "__Ερώτηση__ \n",
    "\n",
    "Μελετώντας <a href=\"https://gym.openai.com/envs/FrozenLake-v0/\">αυτό</a> και βασισμένοι στα συμπεράσματα του προηγούμενου ερωτήματος να περιγράψετε σύντομα το πρόβλημα της παγωμένης λίμνης ως πρόβλημα βελτιστοποίησης. Ποιος είναι ο στόχος του agent;\n",
    "    \n",
    "__Aπάντηση__    \n",
    "     \n",
    "   Στο συγκεκριμένο πρόβλημα ο agent ελέγχει τις κινήσεις ενός χαρακτήρα μέσα σε ένα πλέγμα 8χ8 διαστάσεων. Τα κελιά περιέχουν τις τιμές F(frozen),H(Hole),G(final goal), S(start cell). Κάποια κελιά είναι αδύνατον να περπατηθούν όπως τα H γιατί ο χαρακτήρας μας θα πέσει μέσα. Ο agent μπορεί να κινηθεί μόνο σε F κελιά επειδή είναι παγωμένα. Ο agent μπορεί να κάνει οριζόντιες ή κάθετες κινήσεις στο πλέγμα αλλά όχι διαγώνιες. Ξεκινάει από το κελί S και έχει σαν στόχο να φτάσει στο κελί G. O agent βραβεύεται όταν βρίσκει ένα καλό path για το G κελί. Eπομένως ο στόχος του είναι να βρει το optimal policy, δηλαδή ποια κατεύθυνση να διαλέξει σε κάθε κατάσταση για να φτάσει στο επιθυμητό αποτέλεσμα για να μεγιστοποιήσει τα rewards. Να σημειωθεί πως οι κινήσεις του agent είναι στοχαστικές και όχι ντετερμινιστικές, δηλαδή έχει πιθανότητα 1/3 να πάει εκεί που θέλει,καθώς τα κελιά \"γλιστράνε\" επομένως αλλάζει κατεύθυνση."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "__Ερώτηση__\n",
    "\n",
    "Να διατυπώσετε την ιδιότητα <i>Markov</i>. Πώς απλοποιεί η ιδιότητα αυτή τη μελέτη του συγκεκριμένου προβλήματος;\n",
    "\n",
    "__Aπάντηση__  \n",
    "\n",
    "Μια στοχαστική διαδικασία έχει την ιδιότητα Markov εάν η δεσμευμένη πιθανοτήτα κατανομής των μελλοντικών καταστάσεων της διαδικασίας εξαρτάται μόνο από την παρούσα κατάσταση και όχι από την ακολουθία των γεγονότων που προηγήθηκαν.\n",
    "\n",
    "<img src=\"mp.png\" />\n",
    "\n",
    "Επομένως η παραπάνω ιδιότητα μας απλοποιεί το πρόβλημα καθώς κοιτάμε μόνο την κατάσταση που βρισκόμαστε και όχι όλες τις προηγούμενες(memoryless system).\n",
    "\n",
    "Eπομένως τα transition models ή reward models έχουν την παρακάτω μορφή:\n",
    "\n",
    "<img src=\"models.png\" /> \n",
    "όπου a -> actions in A(set of actions) και s -> states in S (set of states).\n",
    "\n",
    "Το Α λαμβάνει τις τιμές {0,1,2,3}.\n",
    "\n",
    "Το S λαμβάνει τις τιμές {0,1,2...,63}\n",
    "\n",
    "Σε άλλη περίπτωση θα είχαμε ένα πολύπλοκο μοντέλο το οποίο θα ήταν δύσκολο να υπολογιστεί.\n",
    "\n",
    "__Ερώτηση__\n",
    "\n",
    "Να περιγράψετε σύντομα τους αλγορίθμους <i>Policy Iteration</i> και <i>Value Iteration</i>, δίνοντας έμφαση στο διαφορετικό τρόπο με τον οποίο προσεγγίζουν την επίλυση του προβλήματος. Είναι εγγυημένο ότι οι δύο αλγόριθμοι θα συγκλίνουν στη βέλτιστη πολιτική; Αν ναι, οδηγούν σε ίδια ή διαφορετική βέλτιστη πολιτική;\n",
    "\n",
    "__Aπάντηση__  \n",
    "\n",
    "Πριν την απάντηση καλό θα ήταν να ορίσουμε κάποιες παραμέτρους.\n",
    "\n",
    "<h6><i>Policy</i></h6> είναι ο τρόπος με τον οποίο ένας agent διαλέγει ένα action. Γενικά η συνάρτηση policy δέχεται καταστάσεις και επιστρέφει actions. Στο πρόβλημα μας το policy είναι μεγέθους 64 και αποτελείται απο actions για κάθε state π.χ. π = [0,1,3,....2]\n",
    "\n",
    "<img src=\"policy.png\" />\n",
    "\n",
    "<h6><i>Total Reward</i></h6> είναι οι ανταμοιβές r μέχρι ένα timestep T (r_1 + r_2+...r_T). Όπου τα reward έχουν τιμές 1 για μετάβαση σε GOAL state και 0 αλλιώς. Tο total discounted  award καθορίζεται από τον παρακάτω τύπο όπου όσο πιο μεγάλο γ έχουμε σημαίνει πως μας ενδιαφέρουν τα μελλοντικά rewards πάρα πολύ , ενώ όσο πιο χαμηλό είναι μας ενδιαφέρουν τα immediate rewards. Στην περίπτωση μας έχουμε θέση γ=1.0 , οπότε δε χρειάζεται στον τύπο\n",
    "\n",
    "<img src=\"tdr.png\" /> \n",
    "\n",
    "<h6><i>Transition Probabilities</i></h6> επιπλέον ορίζουμε τις πιθανότητες μετάβασης ως εξής. Για ένα next state S_next δεδομένου ενός State S και ενός action a έχουμε : P(S_nexT| S,a) = 1/3. Όμως μπορεί να πάει και σε άλλες 2 καταστάσεις με πιθανότητα 1/3 για παράδειγμα, δηλαδή αν η κίνηση που θέλουμε είναι να πάμε δεξιά, τότε: P(s_right|s,a) = P(s_left|s,a) = P(s_down|s,a) = 1/3. Βλέπουμε πως το πρόγραμμα αφαιρεί μια κατάσταση ώστε να αθροίζουν οι πιθανότητες στο 1. Αυτό φαίνεται από την εντολή __env.P[s][a]__ που επιστρέφει 3 tuples ένα για κάθε πιθανό state που μπορεί να πάει από την S, τα οποία tuples περιέχουν την πιθανότητα μετάβασης στην νέα κατάσταση, τη νέα κατάσταση , το reward και μια boolean τιμή για το αν έγινε η κίνηση ή όχι(done).\n",
    "\n",
    "π.χ. print(env.P[62][3]) \n",
    "\n",
    "outputs: [(0.3333333333333333, 63, 1.0, True), (0.3333333333333333, 54, 0.0, True), (0.3333333333333333, 61, 0.0, False)]\n",
    "    \n",
    "\n",
    "\n",
    "<h6><i>Value Function</i></h6> Δείχνει πόσο καλή είναι μια κατάσταση για έναν agent ώστε να είναι εκεί. Αλλιώς ορίζεται ως το Expected Total Reward για κάθε στοιχείο αν βρίσκεται στην κατάσταση S. Έπειτα από πράξεις καταλήγουμε:\n",
    "\n",
    "<img src=\"bellmaneq.png\" /> , όπου R(s,a) είναι το immediate reward και το άλλο σκέλος είναι το discounted expected reward πηγάινοντας στο s'.\n",
    "\n",
    "Η παραπάνω συνάρτηση μεγιστοποιείται βρίσκοντας το βέλτιστο policy __π*__  δηλαδή τα __α__ (actions) που θα φέρουν το μέγιστο κέρδος.\n",
    "\n",
    "<h6><i>Q function</i></h6>Τέλος να αναφέρουμε τη συνάρτηση __Q:SXA -> R__ , δηλαδή λαμβάνει ένα action και το state που είναι και επιστρέφει μια πραγματική τιμή. Aυτή η πραγματική τιμή συμβολίζει το expected total reward όταν είσαι στη κατάσταση __S__ και διαλέγεις action __α__. Επομένως η V ορίζεται και με τους παρακάτω τρόπους\n",
    "\n",
    "__V*(s) = max{Q*(s,a)}__, over a, for every s    ή     __π* = arg max {Q*(s,a)}__, over a, for every s\n",
    "\n",
    " \n",
    "<h3>Value iteration:</h3>Υπολογίζει την βέλτιστη κατάσταση για τη value function, βελτιώνοντας επαναληπτικά την εκτίμηση της στην κατάσταση S (V(s)). Ενημερώνει επανειλημμένα τις τιμές Q (s, a) και V (s) έως ότου συγκλίνουν. Ο αλγόριθμος αυτός είναι σίγουρο ότι συγκλίνει σε βέλτιστες τιμές.\n",
    "\n",
    "παρακάτω φαίνεται ο ψευδοκώδικας.\n",
    "<img src=\"vi.png\" />\n",
    "\n",
    "Βλέπουμε πως πρώτο βήμα έχει την αρχικοποιήση τυχαία των τιμών της __V(s)__. Έπειτα, προσπαθεί να μεγιστοποιήσει τη συνάρτηση __Q__ επαναλαμβάνοντας τον εξής υπολογισμό για έναν ορισμένο αριθμό επαναλήψεων.\n",
    "Να βρει δηλαδή για κάθε state s το α το οποίο μεγιστοποιεί την __Q__. Ουσιαστικά, βρίσκει το __V*__ κάνοντας διερεύνηση(exploration) όλο το χώρο καταστάσεων και ενεργείων. Έπειτα έχοντας το __optimal V(V*)__ μπορεί να κάνει extract το αντίστοιχο policy κάνοντας χρήση της μεθόδου __extract policy__. Η οποία μέθοδος ουσιαστικά επιλύει την εξισωση : __π* = arg max {Q*(s,a)}, over a, for every s__.\n",
    "\n",
    "\n",
    "<h3>policy iteration:</h3> Ο συγκεκριμένος αλγόριθμος αντί να βελτιώνει την εκτίμηση της value function, αναπροσδιορίζει το policy σε κάθε βήμα και υπολογίζει σύμφωνα με αυτό τις τιμές v(s) μέχρις ότου να συγκλίνει  σε ένα π. Και αυτός ο αλγόριθμος είναι σίγουρο ότι συγκλίνει σε βέλτιστες τιμές.\n",
    "\n",
    "παρακάτω φαίνεται ο ψευδοκώδικας,όπου E[r|s,a] το immediate reward (R(s,a)).\n",
    "<img src=\"pi.png\" /> \n",
    "\n",
    "Παρατηρούμε πως πρώτο βήμα είναι η αρχικοποιήσει ενός policy(μια ακολουθία π με 64 actions ένα για κάθε κατάσταση) με τυχαίο τρόπο.Αυτό το κάνει η συνάρτηση __policy iteration__. Έπειτα η συνάρτηση αυτή καλεί την __compute_policy_v__ η οποία υπολογίζει το __V(s)__ για κάθε state λύνοντας αυτήν την εξίσωση:\n",
    "\n",
    "__V*(s) = max{Vπ(s)}__, over π, for every s ή __Vπ(s) = Σ(1/3)*(r_i + Vπ(s'))__ , for all s', over a, for every s.\n",
    "\n",
    "Στη συνέχεια καλείται η __extract policy__ μέθοδος που βρίσκεται μέσα στην __policy iteration__. Aυτή η συνάρτηση ουσιαστικά επιλύει την εξίσωση __π* = arg max {Vπ(s)}__, over π, for every s.\n",
    "\n",
    "Οι παραπάνω διαδικασίες __compute_policy_v__ και __extract policy__ τρέχουν ουσιαστικά εναλλάξ για ένα αριθμό max_iterations, εώς ότου συγκλίνουν τα old_π με τα new_π. (if (np.all(policy == new_policy))).\n",
    "\n",
    "<h6>Πως διαφέρουν οι δύο αλγόριθμοι </h6>\n",
    "<i>η διαφορά των 2 αλγορίθμων είναι πως ο ένας προσπαθεί να μεγιστοποιήσει το V(s) και στη συνέχεια από αυτό να εξάγει τo βέλτιστο policy. Ενώ ο policy iteration αλγόριθμος προσπαθεί αμέσως να βρει το βέλτιστο π, χωρίς να βρει το βέλτιστο V(s). Ουσιαστικά πετυχάινουν τον ίδιο σκοπό με άλλο τρόπο. Ο value iteration ανγακάζεται ουσιαστικά να κάνει ένα παραπάνω βήμα από τον policy iteration.</i>\n",
    "\n",
    "<h6> Oδηγούν σε ίδια ή διαφορετική βέλτιστη πολιτική; </h6>\n",
    "Oι δύο αλγόριθμοι βρίσκουν βέλτιστες τιμές, ωστόσο ο αλγόριθμος policy iteration συγκλίνει ταχύτερα, λόγω του ότι κάνει λιγότερες επαναλήψεις. Ωστόσο κάθε επανάληψη του είναι πιο αργή-περίπλοκη από του value iteration.\n",
    "\n",
    "__Ερώτηση__\n",
    "\n",
    "Να εκτελέσετε τα προγράμματα που σας δίνονται, τα οποία επιλύουν το\n",
    "πρόβλημα της παγωμένης λίμνης, χρησιμοποιώντας τους αλγορίθμους <i>Policy\n",
    "Iteration</i> και <i>Value Iteration</i> αντίστοιχα. Ποια μέθοδος συγκλίνει στη βέλτιστη λύση σε λιγότερα βήματα; Τι συμπέρασμα βγάζετε; Να ελέγξετε το χρόνο εκτέλεσης του κάθε προγράμματος, χρησιμοποιώντας την εντολή <i>time</i>. Τι συμπέρασμα βγάζετε ως προς την πολυπλοκότητα του κάθε αλγορίθμου;\n",
    "\n",
    "__Aπάντηση__  \n",
    "\n",
    "Η μέθοδος policy iteration συγκλίνει σε μόλις 13 βήματα ενώ η value iteration σε 2357. Επομένως, παρατηρούμε πως ο 1ος αλγόριθμος είναι καλύτερος ως προς τον αριθμό των βημάτων.\n",
    "\n",
    "Ωστόσο η μέθοδος value iteration ήθελε λιγότερα δευτερόλεπτα για να συγκλίνει. Γεγονός που σημαίνει ότι μπορεί να κάνει πολλές επαναλήψεις αλλά είναι πάρα πολύ γρήγορες, ενώ η μέθοδος policy iteration κάνει λίγες επαναλήψεις αλλά αργές. Οι δύο αλγόριθμοι σύγκλιναν στο ίδιο average score = 1.0.\n",
    "\n",
    "Είναι εμφανές πως ο αλγόριθμος policy iteration είναι αρκετά πιο πολύπλοκος στη λύση του από τον value-iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S6mci5P4HJ_1"
   },
   "source": [
    "<h2><b>Policy Iteration</b></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_43MjfJ9G8v7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy-Iteration converged at step 12.\n",
      "--- 2.924541473388672 seconds ---\n",
      "Average scores =  1.0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Solving FrozenLake8x8 environment π* = arg max {Q*(s,a)}, over a, for every s using Policy iteration.\n",
    "Author : Moustafa Alzantot (malzantot@ucla.edu)\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import wrappers\n",
    "import time\n",
    "\n",
    "def run_episode(env, policy, gamma = 1.0, render = False):\n",
    "    \"\"\" Runs an episode and return the total reward \"\"\"\n",
    "    obs = env.reset()\n",
    "    total_reward = 0\n",
    "    step_idx = 0\n",
    "    while True:\n",
    "        if render:\n",
    "            env.render()\n",
    "        obs, reward, done , _ = env.step(int(policy[obs]))\n",
    "        total_reward += (gamma ** step_idx * reward)\n",
    "        step_idx += 1\n",
    "        if done:\n",
    "            break\n",
    "    return total_reward\n",
    "\n",
    "\n",
    "def evaluate_policy(env, policy, gamma = 1.0, n = 100):\n",
    "    scores = [run_episode(env, policy, gamma, False) for _ in range(n)]\n",
    "    return np.mean(scores)\n",
    "\n",
    "def extract_policy(v, gamma = 1.0):\n",
    "    \"\"\" Extract the policy given a value-function \"\"\"\n",
    "    policy = np.zeros(env.nS)\n",
    "    for s in range(env.nS):\n",
    "        q_sa = np.zeros(env.nA)\n",
    "        for a in range(env.nA):\n",
    "            q_sa[a] = sum([p * (r + gamma * v[s_]) for p, s_, r, _ in  env.P[s][a]])\n",
    "        policy[s] = np.argmax(q_sa)\n",
    "    return policy\n",
    "\n",
    "def compute_policy_v(env, policy, gamma=1.0):\n",
    "    \"\"\" Iteratively evaluate the value-function under policy.\n",
    "    Alternatively, we could formulate a set of linear equations in iterms of v[s] \n",
    "    and solve them to find the value function.\n",
    "    \"\"\"\n",
    "    v = np.zeros(env.nS)\n",
    "    eps = 1e-10\n",
    "    while True:\n",
    "        prev_v = np.copy(v)\n",
    "        for s in range(env.nS):\n",
    "            policy_a = policy[s]\n",
    "            v[s] = sum([p * (r + gamma * prev_v[s_]) for p, s_, r, _ in env.P[s][policy_a]])\n",
    "        if (np.sum((np.fabs(prev_v - v))) <= eps):\n",
    "            # value converged\n",
    "            break\n",
    "    return v\n",
    "\n",
    "def policy_iteration(env, gamma = 1.0):\n",
    "    \"\"\" Policy-Iteration algorithm \"\"\"\n",
    "    policy = np.random.choice(env.nA, size=(env.nS))  # initialize a random policy , 64 shape with 0-3 values for up,down,left,right\n",
    "    max_iterations = 200000\n",
    "    gamma = 1.0\n",
    "    for i in range(max_iterations):\n",
    "        old_policy_v = compute_policy_v(env, policy, gamma)\n",
    "        #print(policy) \n",
    "        #print(old_policy_v,old_policy_v.shape) V[s] values\n",
    "        new_policy = extract_policy(old_policy_v, gamma) # policy - moves to do\n",
    "        #print(new_policy,new_policy.shape)# 64 array, each cell with 0 to 3 for each kind of move]\n",
    "        #print(new_policy)\n",
    "        if (np.all(policy == new_policy)):# the initial policy converged to new policy\n",
    "            print ('Policy-Iteration converged at step %d.' %(i+1))\n",
    "            break\n",
    "        policy = new_policy\n",
    "    return policy\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    env_name  = 'FrozenLake8x8-v0'\n",
    "    env = gym.make(env_name)\n",
    "    env = env.unwrapped# https://stackoverflow.com/questions/53836136/why-unwrap-an-openai-gym\n",
    "    start_time = time.time()\n",
    "    optimal_policy = policy_iteration(env, gamma = 1.0)\n",
    "    scores = evaluate_policy(env, optimal_policy, gamma = 1.0)\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "    print('Average scores = ', np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 2, 3, 3, 0, 1, 3, 1, 2, 3, 2, 1, 0, 3, 2, 1, 0, 0, 2, 0, 3,\n",
       "       2, 2, 3, 1, 1, 0, 2, 3, 2, 2, 2, 1, 2, 1, 1, 1, 3, 1, 2, 0, 1, 0,\n",
       "       2, 1, 2, 1, 3, 2, 3, 1, 1, 0, 1, 1, 3, 0, 3, 3, 3, 2, 2, 1])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#policy = np.random.choice(env.nA, size=(env.nS))\n",
    "#policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.3333333333333333, 63, 1.0, True), (0.3333333333333333, 54, 0.0, True), (0.3333333333333333, 61, 0.0, False)]\n",
      "----------------\n",
      "0.3333333333333333\n",
      "63\n",
      "1.0\n",
      "True\n",
      "----------------\n",
      "0.3333333333333333\n",
      "54\n",
      "0.0\n",
      "True\n",
      "----------------\n",
      "0.3333333333333333\n",
      "61\n",
      "0.0\n",
      "False\n",
      "----------------\n"
     ]
    }
   ],
   "source": [
    "# state 2 - action 0\n",
    "print(env.P[62][3])\n",
    "print(\"----------------\")\n",
    "for p, s_, r, underscore in env.P[62][3] :\n",
    "    print(p) # probability\n",
    "    print(s_) # next_state\n",
    "    print(r) # reward\n",
    "    print( underscore)\n",
    "    print(\"----------------\")\n",
    "# Reward is 1 when next state is Final and, 0 otherwise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gcikBq6BHRQM"
   },
   "source": [
    "<h2><b>Value Iteration</b></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gHvcnTDcHGmH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value-iteration converged at iteration# 2357.\n",
      "--- 2.626422882080078 seconds ---\n",
      "Policy average score =  1.0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Solving FrozenLake8x8 environment using Value-Itertion.\n",
    "Author : Moustafa Alzantot (malzantot@ucla.edu)\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import wrappers\n",
    "import time\n",
    "\n",
    "def run_episode(env, policy, gamma = 1.0, render = False):\n",
    "    \"\"\" Evaluates policy by using it to run an episode and finding its\n",
    "    total reward.\n",
    "    args:\n",
    "    env: gym environment.\n",
    "    policy: the policy to be used.\n",
    "    gamma: discount factor.\n",
    "    render: boolean to turn rendering on/off.\n",
    "    returns:\n",
    "    total reward: real value of the total reward recieved by agent under policy.\n",
    "    \"\"\"\n",
    "    obs = env.reset()\n",
    "    total_reward = 0\n",
    "    step_idx = 0\n",
    "    while True:\n",
    "        if render:\n",
    "            env.render()\n",
    "        obs, reward, done , _ = env.step(int(policy[obs]))\n",
    "        total_reward += (gamma ** step_idx * reward)\n",
    "        step_idx += 1\n",
    "        if done:\n",
    "            break\n",
    "    return total_reward\n",
    "\n",
    "\n",
    "def evaluate_policy(env, policy, gamma = 1.0,  n = 100):\n",
    "    \"\"\" Evaluates a policy by running it n times.\n",
    "    returns:\n",
    "    average total reward\n",
    "    \"\"\"\n",
    "    scores = [\n",
    "            run_episode(env, policy, gamma = gamma, render = False)\n",
    "            for _ in range(n)]\n",
    "    return np.mean(scores)\n",
    "\n",
    "def extract_policy(v, gamma = 1.0):\n",
    "    \"\"\" Extract the policy given a value-function \"\"\"\n",
    "    policy = np.zeros(env.nS)\n",
    "    for s in range(env.nS):\n",
    "        q_sa = np.zeros(env.action_space.n)\n",
    "        for a in range(env.action_space.n):\n",
    "            for next_sr in env.P[s][a]:\n",
    "                # next_sr is a tuple of (probability, next state, reward, done)\n",
    "                p, s_, r, _ = next_sr\n",
    "                q_sa[a] += (p * (r + gamma * v[s_]))\n",
    "        policy[s] = np.argmax(q_sa)\n",
    "    return policy\n",
    "\n",
    "\n",
    "def value_iteration(env, gamma = 1.0):\n",
    "    \"\"\" Value-iteration algorithm \"\"\"\n",
    "    v = np.zeros(env.nS)  # initialize value-function\n",
    "    max_iterations = 100000\n",
    "    eps = 1e-20\n",
    "    for i in range(max_iterations):\n",
    "        prev_v = np.copy(v)\n",
    "        for s in range(env.nS):\n",
    "            q_sa = [sum([p*(r + prev_v[s_]) for p, s_, r, _ in env.P[s][a]]) for a in range(env.nA)] \n",
    "            v[s] = max(q_sa)\n",
    "        if (np.sum(np.fabs(prev_v - v)) <= eps):\n",
    "            print ('Value-iteration converged at iteration# %d.' %(i+1))\n",
    "            break\n",
    "    return v\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    env_name  = 'FrozenLake8x8-v0'\n",
    "    gamma = 1.0\n",
    "    env = gym.make(env_name)\n",
    "    env = env.unwrapped\n",
    "    start_time = time.time()\n",
    "    optimal_v = value_iteration(env, gamma);\n",
    "    policy = extract_policy(optimal_v, gamma)\n",
    "    policy_score = evaluate_policy(env, policy, gamma, n=1000)\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "    print('Policy average score = ', policy_score)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Stochastic_Processes_&_Optimization_in_Machine_Learning_(Lab_5_Markov_Decision_Processes).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
